"""
æµ‹è¯•CBAMæ³¨æ„åŠ›æœºåˆ¶åŠŸèƒ½
"""
import torch
import torch.nn as nn
import numpy as np
from models.DenseNet.DenseNet import densenet121, CBAM, ChannelAttention, SpatialAttention
from models.DenseNet.DenseNetConfig import DenseNetConfig

def test_channel_attention():
    """æµ‹è¯•é€šé“æ³¨æ„åŠ›æ¨¡å—"""
    print("=== æµ‹è¯•é€šé“æ³¨æ„åŠ›æ¨¡å— ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    channels = 64
    height, width = 16, 16
    
    # åˆ›å»ºé€šé“æ³¨æ„åŠ›æ¨¡å—
    ca = ChannelAttention(in_planes=channels, ratio=16)
    
    print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: ({batch_size}, {channels}, {height}, {width})")
    print(f"é€šé“ç¼©æ”¾æ¯”ä¾‹: 16")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_tensor = torch.randn(batch_size, channels, height, width)
    attention_weights = ca(input_tensor)
    
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡èŒƒå›´: [{attention_weights.min().item():.4f}, {attention_weights.max().item():.4f}]")
    print(f"æ³¨æ„åŠ›æƒé‡å‡å€¼: {attention_weights.mean().item():.4f}")
    
    # éªŒè¯æ³¨æ„åŠ›æƒé‡åœ¨[0,1]èŒƒå›´å†…
    assert attention_weights.min() >= 0 and attention_weights.max() <= 1, "æ³¨æ„åŠ›æƒé‡åº”åœ¨[0,1]èŒƒå›´å†…"
    print("âœ“ é€šé“æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•é€šè¿‡")

def test_spatial_attention():
    """æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    print("\n=== æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›æ¨¡å— ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    channels = 64
    height, width = 16, 16
    
    # åˆ›å»ºç©ºé—´æ³¨æ„åŠ›æ¨¡å—
    sa = SpatialAttention(kernel_size=7)
    
    print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: ({batch_size}, {channels}, {height}, {width})")
    print(f"å·ç§¯æ ¸å¤§å°: 7")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_tensor = torch.randn(batch_size, channels, height, width)
    attention_weights = sa(input_tensor)
    
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡èŒƒå›´: [{attention_weights.min().item():.4f}, {attention_weights.max().item():.4f}]")
    print(f"æ³¨æ„åŠ›æƒé‡å‡å€¼: {attention_weights.mean().item():.4f}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, 1, height, width)
    assert attention_weights.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶åº”ä¸º{expected_shape}"
    
    # éªŒè¯æ³¨æ„åŠ›æƒé‡åœ¨[0,1]èŒƒå›´å†…
    assert attention_weights.min() >= 0 and attention_weights.max() <= 1, "æ³¨æ„åŠ›æƒé‡åº”åœ¨[0,1]èŒƒå›´å†…"
    print("âœ“ ç©ºé—´æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•é€šè¿‡")

def test_cbam_module():
    """æµ‹è¯•å®Œæ•´çš„CBAMæ¨¡å—"""
    print("\n=== æµ‹è¯•CBAMæ¨¡å— ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    channels = 64
    height, width = 16, 16
    
    # åˆ›å»ºCBAMæ¨¡å—
    cbam = CBAM(in_planes=channels, ratio=16, kernel_size=7)
    
    print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: ({batch_size}, {channels}, {height}, {width})")
    print(f"é€šé“ç¼©æ”¾æ¯”ä¾‹: 16")
    print(f"ç©ºé—´å·ç§¯æ ¸å¤§å°: 7")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_tensor = torch.randn(batch_size, channels, height, width)
    output_tensor = cbam(input_tensor)
    
    print(f"è¾“å‡ºç‰¹å¾å½¢çŠ¶: {output_tensor.shape}")
    print(f"è¾“å…¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦ç›¸åŒ: {input_tensor.shape == output_tensor.shape}")
    
    # è®¡ç®—æ³¨æ„åŠ›æ•ˆæžœ
    input_mean = input_tensor.mean().item()
    output_mean = output_tensor.mean().item()
    print(f"è¾“å…¥ç‰¹å¾å‡å€¼: {input_mean:.4f}")
    print(f"è¾“å‡ºç‰¹å¾å‡å€¼: {output_mean:.4f}")
    print(f"ç‰¹å¾å˜åŒ–æ¯”ä¾‹: {(output_mean - input_mean) / input_mean * 100:.2f}%")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output_tensor.shape == input_tensor.shape, "CBAMè¾“å‡ºå½¢çŠ¶åº”ä¸Žè¾“å…¥ç›¸åŒ"
    print("âœ“ CBAMæ¨¡å—æµ‹è¯•é€šè¿‡")

def test_densenet_with_cbam():
    """æµ‹è¯•å¸¦CBAMçš„DenseNetæ¨¡åž‹"""
    print("\n=== æµ‹è¯•DenseNet + CBAM ===")
    
    config = DenseNetConfig()
    
    # åˆ›å»ºå¸¦CBAMçš„æ¨¡åž‹
    model_with_cbam = densenet121(
        num_class=7,
        use_cbam=True,
        use_dynamic_reuse=False  # ä¸“æ³¨æµ‹è¯•CBAM
    )
    
    # åˆ›å»ºä¸å¸¦CBAMçš„æ¨¡åž‹
    model_without_cbam = densenet121(
        num_class=7,
        use_cbam=False,
        use_dynamic_reuse=False
    )
    
    # è®¡ç®—å‚æ•°é‡
    params_with_cbam = sum(p.numel() for p in model_with_cbam.parameters())
    params_without_cbam = sum(p.numel() for p in model_without_cbam.parameters())
    
    print(f"ä¸å¸¦CBAMçš„å‚æ•°é‡: {params_without_cbam:,}")
    print(f"å¸¦CBAMçš„å‚æ•°é‡: {params_with_cbam:,}")
    print(f"CBAMå‚æ•°å¢žåŠ é‡: {params_with_cbam - params_without_cbam:,}")
    print(f"å‚æ•°å¢žåŠ æ¯”ä¾‹: {(params_with_cbam - params_without_cbam) / params_without_cbam * 100:.2f}%")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    test_input = torch.randn(batch_size, 1, 48, 48)  # ç°åº¦å›¾åƒ
    
    print(f"\næµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # æµ‹è¯•å¸¦CBAMçš„æ¨¡åž‹
    with torch.no_grad():
        output_with_cbam = model_with_cbam(test_input)
    print(f"å¸¦CBAMè¾“å‡ºå½¢çŠ¶: {output_with_cbam.shape}")
    print(f"å¸¦CBAMè¾“å‡ºèŒƒå›´: [{output_with_cbam.min().item():.4f}, {output_with_cbam.max().item():.4f}]")
    
    # æµ‹è¯•ä¸å¸¦CBAMçš„æ¨¡åž‹
    with torch.no_grad():
        output_without_cbam = model_without_cbam(test_input)
    print(f"ä¸å¸¦CBAMè¾“å‡ºå½¢çŠ¶: {output_without_cbam.shape}")
    print(f"ä¸å¸¦CBAMè¾“å‡ºèŒƒå›´: [{output_without_cbam.min().item():.4f}, {output_without_cbam.max().item():.4f}]")
    
    # æ¯”è¾ƒè¾“å‡ºå·®å¼‚
    output_diff = torch.abs(output_with_cbam - output_without_cbam).mean().item()
    print(f"è¾“å‡ºå·®å¼‚å‡å€¼: {output_diff:.4f}")
    
    print("âœ“ DenseNet CBAMæµ‹è¯•é€šè¿‡")

def test_cbam_parameters():
    """æµ‹è¯•ä¸åŒCBAMå‚æ•°çš„å½±å“"""
    print("\n=== æµ‹è¯•ä¸åŒCBAMå‚æ•° ===")
    
    configs = [
        {"ratio": 8, "kernel_size": 3, "name": "ratio_8_kernel_3"},
        {"ratio": 16, "kernel_size": 7, "name": "ratio_16_kernel_7"},
        {"ratio": 32, "kernel_size": 3, "name": "ratio_32_kernel_3"},
        {"ratio": 16, "kernel_size": 3, "name": "ratio_16_kernel_3"},
    ]
    
    test_input = torch.randn(1, 1, 48, 48)
    
    for config in configs:
        print(f"\n--- {config['name']} ---")
        
        # åˆ›å»ºCBAMæ¨¡å—
        cbam = CBAM(in_planes=64, ratio=config['ratio'], kernel_size=config['kernel_size'])
        
        # è®¡ç®—å‚æ•°é‡
        params = sum(p.numel() for p in cbam.parameters())
        print(f"CBAMå‚æ•°é‡: {params:,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_features = torch.randn(1, 64, 24, 24)
        with torch.no_grad():
            output = cbam(test_features)
        
        print(f"è¾“å…¥å½¢çŠ¶: {test_features.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"ç‰¹å¾å˜åŒ–èŒƒå›´: [{(output - test_features).min().item():.4f}, {(output - test_features).max().item():.4f}]")

def test_cbam_attention_visualization():
    """æµ‹è¯•CBAMæ³¨æ„åŠ›å¯è§†åŒ–æ•°æ®"""
    print("\n=== æµ‹è¯•CBAMæ³¨æ„åŠ›æƒé‡ ===")
    
    # åˆ›å»ºCBAMæ¨¡å—
    cbam = CBAM(in_planes=32, ratio=16, kernel_size=7)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.randn(1, 32, 16, 16)
    
    # åˆ†åˆ«èŽ·å–é€šé“å’Œç©ºé—´æ³¨æ„åŠ›æƒé‡
    with torch.no_grad():
        # é€šé“æ³¨æ„åŠ›æƒé‡
        channel_weights = cbam.ca(test_input)
        print(f"é€šé“æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {channel_weights.shape}")
        print(f"é€šé“æ³¨æ„åŠ›ç»Ÿè®¡: å‡å€¼={channel_weights.mean().item():.4f}, æ ‡å‡†å·®={channel_weights.std().item():.4f}")
        
        # åº”ç”¨é€šé“æ³¨æ„åŠ›åŽçš„ç‰¹å¾
        channel_attended = test_input * channel_weights
        
        # ç©ºé—´æ³¨æ„åŠ›æƒé‡
        spatial_weights = cbam.sa(channel_attended)
        print(f"ç©ºé—´æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {spatial_weights.shape}")
        print(f"ç©ºé—´æ³¨æ„åŠ›ç»Ÿè®¡: å‡å€¼={spatial_weights.mean().item():.4f}, æ ‡å‡†å·®={spatial_weights.std().item():.4f}")
        
        # æœ€ç»ˆè¾“å‡º
        final_output = channel_attended * spatial_weights
        
        # åˆ†æžæ³¨æ„åŠ›æ•ˆæžœ
        original_std = test_input.std().item()
        final_std = final_output.std().item()
        print(f"åŽŸå§‹ç‰¹å¾æ ‡å‡†å·®: {original_std:.4f}")
        print(f"CBAMå¤„ç†åŽæ ‡å‡†å·®: {final_std:.4f}")
        print(f"ç‰¹å¾åŒºåˆ†åº¦å˜åŒ–: {(final_std - original_std) / original_std * 100:.2f}%")
        
    print("âœ“ CBAMæ³¨æ„åŠ›å¯è§†åŒ–æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•CBAMæ³¨æ„åŠ›æœºåˆ¶åŠŸèƒ½...\n")
    
    try:
        # æµ‹è¯•å„ä¸ªç»„ä»¶
        test_channel_attention()
        test_spatial_attention()
        test_cbam_module()
        
        # æµ‹è¯•å®Œæ•´æ¨¡åž‹
        test_densenet_with_cbam()
        
        # æµ‹è¯•ä¸åŒå‚æ•°é…ç½®
        test_cbam_parameters()
        
        # æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–
        test_cbam_attention_visualization()
        
        print("\nðŸŽ‰ æ‰€æœ‰CBAMæµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        print("CBAMæ³¨æ„åŠ›æœºåˆ¶å·²æˆåŠŸé›†æˆåˆ°DenseNetä¸­ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
